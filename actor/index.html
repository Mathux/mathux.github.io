<!DOCTYPE html>
<html lang="en">
  <head>
    <title>ACTOR</title>
    <meta name="description" content="Action-Conditioned 3D Human Motion Synthesis with Transformer VAE">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <meta charset="utf-8">

    <!-- ICONS -->
    <script src="https://kit.fontawesome.com/bacac70704.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    <meta name="twitter:card" content="summary_large_image" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://mathis.petrovich.fr/actor/" />
    <meta property="og:title" content="ACTOR" />
    <meta property="og:description" content="Action-Conditioned 3D Human Motion Synthesis with Transformer VAE" />
    <meta property="og:image" content="https://mathis.petrovich.fr/actor/images/small_bigteaser_white.png" />
    <meta property="og:image:type" content="image/png" />
    <meta property="og:image:width" content="1920" />
    <meta property="og:image:height" content="1177" />
    <meta property="og:image:alt" content="Teaser" />

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">
    <link href="css/style.css" rel="stylesheet">
    <link href="css/media.css" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VL12GQY0EE"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-VL12GQY0EE');
    </script>
    <!-- <script src="syncvideo.js"></script> -->
  </head>
  <body>

    <div class="container" style="text-align:center;">
      <div class="row" style="text-align:center">
	<!-- <h1 style="font-family: 'Shadows Into Light', cursive; textsize=2em">TEMOS: </h1> -->
	<h1>Action-Conditioned 3D Human Motion Synthesis with Transformer VAE</h1>
	<!-- <h4>Conference X</h4> style="font-family: 'Shadows Into Light', cursive; -->
      </div>
      <div class="row" style="text-align:center">
	<h4>
	  <a href="../"><nobr>Mathis Petrovich</nobr></a> &emsp;
	  <a href="https://ps.is.mpg.de/~black" target="_blank"><nobr>Michael J. Black</nobr></a> &emsp;
	  <a href="https://imagine.enpc.fr/~varolg" target="_blank"><nobr>G&uumll Varol</nobr></a>
	</h4>
      </div>
      <div class="row" style="text-align:center">
	<h4>
	  <a href="http://iccv2021.thecvf.com/home" target="_blank"><nobr>ICCV 2021 (Oral)</nobr></a> &emsp;
	</h4>
      </div>
    </div>

    <div class="container" style="text-align:center;">
      <div class="row">
	<h3custom style="text-align:center;">
	  <img src="images/bigteaser.png" alt="teaser.png" class="text-center" style="width: 100%; max-width: 1000px;" >
	</h3>
      </div>

      <div class="row">
	<h3custom style="text-align:center;">
	  <a target="_blank" href="https://arxiv.org/abs/2104.05670"> <button type="button" class="btn btn-primary btn-lg"> Paper </button></a>
	  <a target="_blank" href="https://github.com/Mathux/ACTOR"> <button type="button" class="btn btn-primary btn-lg"> Code </button></a>
	  <a target="_blank" href="poster.pdf"> <button type="button" class="btn btn-primary btn-lg"> Poster </button></a>
	  <a target="_blank" href="actor.bib"> <button type="button" class="btn btn-primary btn-lg"> BibTex </button></a>
	</h3>
      </div>
    </div>

    <div class="container">
      <div class="row">
	<div class="col-md-12">
          <h3>Abstract</h3>
          <p style="font-size: 120%;text-align: justify;">
	    We tackle the problem of action-conditioned generation
	    of realistic and diverse human motion sequences.
	    In contrast to methods that complete, or extend, motion sequences,
	    this task does not require an initial pose or sequence. Here
	    we learn an action-aware latent representation for human
	    motions by training a generative variational autoencoder
	    (VAE). By sampling from this latent space and querying
	    a certain duration through a series of positional encodings,
	    we synthesize variable-length motion sequences conditioned
	    on a categorical action. Specifically, we design
	    a Transformer-based architecture, ACTOR, for encoding
	    and decoding a sequence of parametric SMPL human body
	    models estimated from action recognition datasets.
	    We evaluate our approach on the NTU RGB+D, HumanAct12 and
	    UESTC datasets and show improvements over the state of
	    the art. Furthermore, we present two use cases:
	    improving action recognition through adding our synthesized data
	    to training, and motion denoising. Code and models are
	    available on our project page.
	  </p>
	</div>
      </div>

      <div class="row">
	<div class="col-md-12">
          <h3>Video</h3>
	  <center>
	    <iframe src="https://www.youtube.com/embed/De_1NoCXlEw" width="850" height="480" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	  </center>
	</div>
      </div>

      <br>

      <div class="row">
	<div class="col-md-12">
	  <h3>Approach</h3>
	  <center>
	    <img src="images/pipeline.png" alt="pipeline.png" class="center" style="width: 100%; max-width: 1000px">
	  </center>
	</div>
	<div class="col-md-6 col-sm-12 col-xs-12">
	  <p style="font-size: 120%;text-align: justify;">
	    <b> <u>Encoder</u></b>: Given a sequence of body poses
	    <img src="https://latex.codecogs.com/svg.latex?{P_1, \ldots, P_T}" alt="P_1, ..., P_T" border="0" onload="this.style.height=(this.height/8+'ex')">
	    and an action label
	    <img src="https://latex.codecogs.com/svg.latex?{a}" alt="a" border="0" onload="this.style.height=(this.height/8+'ex')">
	    , the encoder outputs
	    distribution parameters on which we define a KL loss
	    <img src="https://latex.codecogs.com/svg.latex?{\mathcal{L}_{KL}}" alt="\mathcal{L}_{KL}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    .
	    We use extra learnable tokens per action (
	    <img src="https://latex.codecogs.com/svg.latex?{\mu_{a}^{token}}" alt="\mu_{a}^{token}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    and
	    <img src="https://latex.codecogs.com/svg.latex?{\Sigma_{a}^{token}}" alt="\Sigma_{a}^{token}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    )
	    as a way to
	    obtain
	    <img src="https://latex.codecogs.com/svg.latex?{\mu}" alt="\mu" border="0" onload="this.style.height=(this.height/8+'ex')">
	    and
	    <img src="https://latex.codecogs.com/svg.latex?{\Sigma}" alt="\Sigma" border="0" onload="this.style.height=(this.height/8+'ex')">
	    from the Transformer encoder.
	    We sample from
	    <img src="https://latex.codecogs.com/svg.latex?{\mu}" alt="\mu" border="0" onload="this.style.height=(this.height/8+'ex')">
	    and
	    <img src="https://latex.codecogs.com/svg.latex?{\Sigma}" alt="\Sigma" border="0" onload="this.style.height=(this.height/8+'ex')">
	    the latent representation
	    <img src="https://latex.codecogs.com/svg.latex?{z \in \mathbf{M}}" alt="z \in \mathbf{M}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    (the encoded input).
	  </p>
	</div>
	<div class="col-md-6 col-sm-12 col-xs-12">
	  <p style="font-size: 120%;text-align: justify;">
	    <b><u>Decoder</u></b>: It takes the latent vector
	    <img src="https://latex.codecogs.com/svg.latex?{z}" alt="z" border="0" onload="this.style.height=(this.height/8+'ex')">, an action label
	    <img src="https://latex.codecogs.com/svg.latex?{a}" alt="a" border="0" onload="this.style.height=(this.height/8+'ex')">, and a duration
	    <img src="https://latex.codecogs.com/svg.latex?{T}" alt="T" border="0" onload="this.style.height=(this.height/8+'ex')">
	    as input.
	    The action determines the learnable
	    <img src="https://latex.codecogs.com/svg.latex?{b_{a}^{token}}" alt="b_{a}^{token}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    additive token,
	    and the duration determines the number of positional encodings (PE) to input to the decoder.
	    The decoder outputs the whole sequence
	    <img src="https://latex.codecogs.com/svg.latex?{\widehat{P}_1, \ldots, \widehat{P}_T}" alt="\widehat{P}_1, \ldots, \widehat{P}_T" border="0" onload="this.style.height=(this.height/8+'ex')">
	    against which
	    the reconstruction loss
	    <img src="https://latex.codecogs.com/svg.latex?{\mathcal{L}_{P}}" alt="\mathcal{L}_{P}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    is computed.
	    In addition, we compute vertices with a differentiable SMPL layer
	    to define a vertex loss
	    <img src="https://latex.codecogs.com/svg.latex?{\mathcal{L}_{V}}" alt="\mathcal{L}_{V}" border="0" onload="this.style.height=(this.height/8+'ex')">
	    .
	  </p>
	</div>

	<div class="col-md-12 col-sm-12 col-xs-12">
	  <p style="font-size: 120%;text-align: justify;">
	    <b><u>Generator</u></b>: The decoder alone is simply the generator, where the encoded input <img src="https://latex.codecogs.com/svg.latex?{z}" alt="z" border="0" onload="this.style.height=(this.height/8+'ex')"> is replaced by a vector randomly sampled from a Gaussian distribution.
	  </p>
	</div>
      </div>

      <!-- visuals -->
      <h3 class="colored">Visual results</h3>
      <hr/>
      <h4 style="text-align: center;margin-top:1em;margin-bottom:1em"> Generated motions </h4>
      <div class="row">
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/pickup.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/raising_arms.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/torso_stretching.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/bending.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/knee_raising.mp4"></video>
	</div>
      </div>
      <div class="row">
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/salute.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/high_knee_running.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/kicking.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/upper_stretching.mp4"></video>
	</div>
	<div class="col-md-2 col-sm-2 col-xs-2">
	  <video autoplay loop playsinline muted width="450" height="270" src="visuals/jumping_jacks.mp4"></video>
	</div>
      </div>

      <br>

      <div class="row">
	<div class="col-md-12">
	  <p style="font-size: 120%;text-align: justify;">
	    Please refer to the video for more examples and see other experiments (different durations, interpolation in latent space, loss ablation).
	  </p>
	</div>
      </div>

      <div class="row">
	<div class="col-md-12">
	  <h3>Results</h3>
	  <h4 style="text-align: center;margin-top:1em;margin-bottom:0.5em">Comparison to the state of the art</h4>

	  <center>
	    <img src="images/sota.png" alt="sota.png" class="center" style="width: 100%; max-width: 1000px;">
	  </center>

	  <br>

	  <p style="font-size: 120%;text-align: justify;">
	    Please refer to the paper to get more details.
	  </p>
	</div>
      </div>

      <div class="row">
	<div class="col-md-12">
	  <h3>Bibtex</h3>
          <p style="font-size: 120%;text-align: justify;">
	    If you find this project useful for your research, please cite
	  </p>
	  <div class="card">
	    <div class="card-block">
	      <pre class="card-text clickselect">
 @inproceedings{petrovich22temos,
     title = {{TEMOS}: Generating diverse human motions from textual descriptions},
     author = {Petrovich, Mathis and Black, Michael J. and Varol, G{\"u}l},
     booktitle = {European Conference on Computer Vision ({ECCV})},
     year = {2022}
 }</pre>
	    </div>
	  </div> <!--" -->
	</div>
      </div>

      <br>

      <div class="row">
	<div class="col-md-12">
          <h3>Acknowledgements Notice</h3>
          <p style="font-size: 120%;text-align: justify;">
	    This work was granted access to
	    the HPC resources of IDRIS under the allocation 2021-101535 made by GENCI.
	    The authors would like to thank Mathieu Aubry and David Picard
	    for helpful feedback, Chuan Guo and Shihao Zou for their help with Action2Motion details.
	  </p>
	</div>
      </div>

      <div class="row">
	<div class="col-md-12">
          <h3>Copyright Notice</h3>
          <p style="font-size: 120%;text-align: justify;">
	    The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright.
          </p>
	</div>
      </div>
    </div>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
  </body>
</html>
